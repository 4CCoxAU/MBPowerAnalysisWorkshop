---
output: html_document
editor_options: 
  chunk_output_type: console
---


```{r, echo=FALSE}
xfun::embed_file('01-introToSimulation.Rmd')
```

Let's start by loading the required packages again:

```{r setup, include=TRUE}
#if you don't have the pacman package loaded on your computer, uncomment the next line, install pacman, and load in the required packages
#install.packages('pacman')
#load the required packages:
pacman::p_load(knitr, # rendering of .Rmd file
               lme4, # model specification / estimation
               afex, # anova and deriving p-values from lmer
               broom.mixed, # extracting data from model fits
               faux, # generate correlated values
               tidyverse, # data wrangling and visualisation
               ggridges, #visualisation
               viridis, # color schemes for visualisation
               kableExtra, #helps with knitting the .Rmd file
               cowplot, #visualisation tool to include multiple plots
               MASS,
               ggrain)
set.seed(1234)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE, fig.width=12, fig.height=11, fig.fullwidth=TRUE)

plot_theme <- 
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5, size = 18), 
        legend.position = "none", 
        axis.text.x = element_text(size = 13), 
        axis.title.x = element_text(size = 13), 
        axis.text.y = element_text(size = 12), 
        axis.title.y = element_text(size = 13))
```

In the lecture, we introduced the importance of a cumulative science perspective on scientific enquiry and argued that our explorations need to reflect the systematic exploration of the methodological space. Heterogeneity here plays a key role in the evidence that we have available to us.

Let's imagine a scenario where a synthesis of evidence has produced an effect size estimate of 0.35 [XX; XX], just as in Zettersten, Cox, Bergmann et al. (2024). How can we use this to guide our study of IDS preference?

Let's start by taking the simulation function we made in the previous exercise and adapt it to the new scale of effect sizes:

```{r}
# set up the custom data simulation function
SimulateEffectSizeData <- function(
  n_subj = 24,   # number of subjects
  n_ADS  = 8,   # number of ingroup stimuli
  n_IDS =  8,   # number of outgroup stimuli
  beta_0 = 0,   # grand mean
  beta_1 =  0.35,   # effect of category
  omega_0 =  0.05,   # by-item random intercept sd
  tau_0 = 0.1,   # by-subject random intercept sd
  tau_1 =  0.1,   # by-subject random slope sd
  rho = 0.4,   # correlation between intercept and slope
  sigma = 0.5) { # residual (standard deviation)

  items <- data.frame(
  #item_id = seq_len(n_ADS + n_IDS),
  Register = rep(c("IDS", "ADS"), c(n_ADS, n_IDS)),
  O_0i = rnorm(n = n_ADS + n_IDS, mean = 0, sd = omega_0)) %>% 
  mutate(item_id = faux::make_id(nrow(.), "I")) %>%
  mutate(SpeechStyle = recode(Register, "ADS" = -0.5, "IDS" = +0.5))

  # simulate a sample of subjects

# sample from a multivariate random distribution 
  subjects <- faux::rnorm_multi(
  n = n_subj, 
  mu = 0, # means for random effects are always 0
  sd = c(tau_0, tau_1), # set SDs
  r = rho, # set correlation, see ?faux::rnorm_multi
  varnames = c("T_0s", "T_1s")
) %>%
  mutate(subj_id = faux::make_id(nrow(.), "S"))

  ParameterValues <- crossing(subjects, items)  %>%
    mutate(e_si = rnorm(nrow(.), mean = 0, sd = sigma)) %>%
    dplyr::select(subj_id, item_id, Register, SpeechStyle, everything())
  
  EffectSizeDataSimulated <- ParameterValues %>%
    mutate(LT = beta_0 + T_0s + O_0i + (beta_1 + T_1s) * SpeechStyle + e_si) %>%
    dplyr::select(subj_id, item_id, Register, SpeechStyle, LT)
}

EffectSizeDataSimulated <- SimulateEffectSizeData()
```

Let's again plot this data and see if it looks correct:

```{r}
EffectSizeDataSimulated <- SimulateEffectSizeData()

meanADSRT <- mean(filter(EffectSizeDataSimulated, Register == "ADS")$LT)

EffectSizeDataSimulated <- EffectSizeDataSimulated %>%
  mutate(LT = LT - meanADSRT)

dat_sim_plot <- EffectSizeDataSimulated %>%
  group_by(subj_id, Register) %>%
  dplyr::summarise(medLT = mean(LT))

ggplot(aes(x = Register, y = medLT, fill = Register), data = dat_sim_plot) + 
  geom_rain(alpha = 0.8, rain.side = "f1x1", id.long.var = "subj_id", point.args.pos = list(position = position_jitter(width = 0.04, height = 0, seed = 42)), line.args.pos = list(position = position_jitter(width = 0.04, height = 0, seed = 42))) + 
  scale_fill_manual(values = c("#FC4E07", "steelblue")) + 
  ggtitle('Effect Size Differences across Speech Styles') + 
  xlab("Speech Style") + 
  ylab('Effect Size') + 
  scale_color_manual(values = viridis(n = 27)) +
  plot_theme
```

Wonderful, this looks correct! As we saw in the previous exercise sheet, however, our experimental design choices really matter for statistical power. For example, the number of stimulus items (i.e., number of repeated measures per participant) matter hugely, as does number of participants. How can we efficiently explore the multiverse of choices that govern our experimental design?

What if we set up a grid search and allow models to be fit with different parameter values? 

```{r}
run_sims_grid_point <- function(filename_full, trial_n, subj_n) {
  ADS_n = trial_n / 2
  IDS_n = trial_n / 2
  n_subj = subj_n
  
  dataSimulated <- SimulateEffectSizeData(
                        n_subj = n_subj,
                         n_ADS = ADS_n, 
                         n_IDS = IDS_n)
    
  model <- lmer(LT ~ 1 + SpeechStyle + (1 | item_id) + (1 + SpeechStyle | subj_id),
                data = dataSimulated)

  sim_results <- broom.mixed::tidy(model)
  
  # append the results to a file
  append <- file.exists(filename_full)
  write_csv(sim_results, filename_full, append = append)
  
  # return the tidy table
  sim_results
}

reps <- 20
subj_n <- seq(12, 36, by = 12)
trial_n <- seq(4, 16, by = 4)

param_combinations <- expand.grid(subj_n = subj_n, 
                                  trial_n = trial_n)

for (i in seq_len(nrow(param_combinations))) {
  sim_params <- param_combinations[i, ]
  filename_full <- paste0('sims_grid_search/test_grid_search_',
                          sim_params$subj_n, '_',
                          sim_params$trial_n, '.csv')
  start_time <- Sys.time() # Start time
  sims <- purrr::map_df(1:reps, ~run_sims_grid_point(filename_full = filename_full,
                                                         subj_n = sim_params$subj_n,
                                                         trial_n = sim_params$trial_n))
  end_time <- Sys.time() # End time
  cat("Simulation", i, "Time elapsed:", end_time - start_time, "\n")
}

setwd(here())
setwd(here("sims_grid_search"))
file_names <- list.files(pattern = "*.csv")

# read in all CSV files into a list of dataframes
df_list <- purrr::map(file_names, ~{
  df <- read.csv(.x) 
  df$filename <- .x 
  df
  })

df <- purrr::reduce(df_list, dplyr::bind_rows)

df_per_sim <- df %>%
  filter(effect == "fixed") %>%
  filter(term == "SpeechStyle") %>%
  group_by(filename) %>%
  summarise(median_estimate = median(estimate), median_se = median(std.error),
            power = mean(p.value < 0.05))

PowerGridData <- df_per_sim %>%
  mutate(n_subj = sapply(strsplit(filename, "_"), `[`,4),
         n_trial = as.numeric(str_replace(sapply(strsplit(filename, "_"), `[`, 5), pattern = ".csv","")))

ggplot(PowerGridData) +
  geom_point(aes(x = n_subj, y = power)) +
  plot_theme +
  facet_wrap(~n_trial)

```






```{r}
mod_sim <- lmer(LT ~ 1 + SpeechStyle + (1 | item_id) + (1 + SpeechStyle | subj_id), 
                data = EffectSizeDataSimulated)
summary(mod_sim)
```






```{r}
dat_sim <- my_sim_data(n_subj = 30)

dat_sim %>%
  mutate(subj_id = as.factor(subj_id)) %>%
  ggplot() +
  geom_density_ridges(aes(y = reorder(subj_id, RT), x = RT, color = "black", fill = subj_id), show.legend = FALSE, quantile_lines = T, quantiles = c(0.025, 0.5, 0.975), alpha = 0.6, scale = 1.1) +
  ggtitle('Intercept Variability by Subject') +
  geom_vline(xintercept = 0, color = "black", size = 0.3, linetype = "dotted") +
  xlab('DV') +
  ylab('Subject ID') +
  #facet_wrap(~category) +
  scale_color_manual(values=c(viridis(n = 30))) +
  scale_fill_manual(values=c(viridis(n = 30))) +
  plot_theme

dat_sim %>%
  mutate(item_id = as.factor(item_id)) %>%
  ggplot() +
  geom_density_ridges(aes(y = reorder(item_id, RT), x = RT, color = "black", fill = item_id), show.legend = FALSE, quantile_lines = TRUE, quantiles = c(0.025, 0.5, 0.975), alpha = 0.6, scale = 1.1) +
  ggtitle('Intercept Variability by Item') +
  #geom_vline(xintercept = 800, color = "black", size = 0.3, linetype = "dotted") +
  xlab('DV') +
  ylab('Item ID') +
  #facet_wrap(~category) +
  scale_color_manual(values=c(viridis(n = 30))) +
  scale_fill_manual(values=c(viridis(n = 30))) +
  plot_theme
```


```{r}
dat_sim <- my_sim_data(n_subj = 50)
```


```{r}
# fit a linear mixed-effects model to data
mod_sim <- lmer(RT ~ 1 + category_contrast + (1 | item_id) + (1 + category_contrast | subj_id), data = dat_sim)

summary(mod_sim, corr = FALSE)
```


```{r}
# set up the custom data simulation function
my_sim_data <- function(
  n_subj = 50,   # number of subjects
  ADS_n  = 10,   # number of ingroup stimuli
  IDS_n =  10,   # number of outgroup stimuli
  beta_0 = 0,   # grand mean
  beta_1 =  0.35,   # effect of category
  beta_as = 0.8,
  subject_as = 0.05,
  omega_0 =  0.1,   # by-item random intercept sd
  tau_0 = 0.1,   # by-subject random intercept sd
  tau_1 =  0.06,   # by-subject random slope sd
  rho = 0.2,   # correlation between intercept and slope
  sigma = 0.5) { # residual (standard deviation)

  items <- data.frame(
    item_id = seq_len(ADS_n + IDS_n),
    category = rep(c("ADS", "IDS"), c(ADS_n, IDS_n)),
    category_contrast = rep(c(-0.5, 0.5), c(ADS_n, IDS_n)),
    O_0i = rnorm(n = ADS_n + IDS_n, mean = 0, sd = omega_0)) %>%
    mutate(item_id = faux::make_id(nrow(.), "I"))

  # simulate a sample of subjects

# sample from a multivariate random distribution 
  subjects <- faux::rnorm_multi(
  n = n_subj, 
  mu = 0, # means for random effects are always 0
  sd = c(tau_0, tau_1, subject_as), # set SDs
  r = rho, # set correlation, see ?faux::rnorm_multi
  varnames = c("T_0s", "T_1s", "S_as")) %>%
    mutate(subj_id = faux::make_id(nrow(.), "S")) %>%
    mutate(X_a = runif(n_subj, min = -0.5, max = 0.5))
#add subject age measure, sample from distribution from -0.5 to 0.5.
  crossing(subjects, items) %>%
    mutate(e_si = rnorm(nrow(.), mean = 0, sd = sigma),
           RT = beta_0 + T_0s + O_0i + (beta_1 + T_1s) * category_contrast + ((beta_as + S_as) * X_a * category_contrast) + e_si) %>%
    dplyr::select(subj_id, item_id, category, category_contrast, RT, X_a)
}
```

```{r}
dat_sim <- my_sim_data()

dat_sim %>%
ggplot() + 
  geom_point(aes(y = RT, x = X_a, color = subj_id), position = "jitter", alpha = 0.6, size = 1) + 
  geom_smooth(method = "lm", se = TRUE, formula = y ~ x, aes(y = RT, x = X_a)) +
  ggtitle("Age") +
  xlab("Age") + 
  facet_wrap(~category_contrast) +
  plot_theme

dat_sim <- my_sim_data()
mod_sim <- lmer(RT ~ 1 + category_contrast*X_a + (1 | item_id) + (1 + category_contrast | subj_id), data = dat_sim)
summary(mod_sim, corr = FALSE)
```


Mixed-effects modeling is a powerful technique for analyzing data from complex designs. The technique is close to ideal for analyzing data with crossed random factors of subjects and stimuli: it gracefully and simultaneously accounts for subject and item variance within a single analysis, and outperforms traditional techniques in terms of type I error and power (Barr et al. 2013). However, this additional power comes at the price of technical complexity.

Here we only considered a design with a normally distributed response variable. However, generalised linear mixed effect models allow for response variables with different distributions, such as binomial. Our supplemental online materials (https://osf.io/3cz2e/) illustrate the differences in simulation required for the study design in this paper with a binomial accuracy score (correct/incorrect) as the response variable.


```{r}
# Set parameters for the ex-Gaussian distribution
mu <- 500  # Mean of the Gaussian component
sigma <- 50  # Standard deviation of the Gaussian component
lambda <- 0.001  # Rate parameter of the exponential component

# Number of samples to generate
n <- 1000

# Generate random numbers from a Gaussian distribution
gaussian_part <- rnorm(n, mean = mu, sd = sigma)

# Generate random numbers from an exponential distribution
exponential_part <- rexp(n, rate = lambda)

# Combine the two distributions to create the ex-Gaussian distribution
reaction_times <- gaussian_part + exponential_part

# Plot the histogram of simulated reaction times
hist(try$RT, breaks = 30, main = "Simulated Reaction Times (Ex-Gaussian)", xlab = "Reaction Time (ms)")
try <- my_sim_data()

try %>%
  ggplot() +
  geom_density(aes(x = RT, fill = as.factor(category_contrast))) +
  #xlim(c(2000, 20000)) +
  plot_theme



# Summary of the model
summary(model)
exp(6.43447 + 0.18226) - exp(6.43447)

# set up the custom data simulation function
my_sim_data <- function(
  n_subj = 50,   # number of subjects
  ADS_n  = 10,   # number of ingroup stimuli
  IDS_n =  10,   # number of outgroup stimuli
  beta_0 = 10000,   # grand mean
  beta_1 =  2000,   # effect of category
  omega_0 =  200,   # by-item random intercept sd
  tau_0 = 200,   # by-subject random intercept sd
  tau_1 =  200,   # by-subject random slope sd
  rho = 0.2,   # correlation between intercept and slope
  sigma = 1000) { # residual (standard deviation)

  items <- data.frame(
    item_id = seq_len(ADS_n + IDS_n),
    category = rep(c("ADS", "IDS"), c(ADS_n, IDS_n)),
    category_contrast = rep(c(-0.5, 0.5), c(ADS_n, IDS_n)),
    O_0i = rnorm(n = ADS_n + IDS_n, mean = 0, sd = omega_0))

  # simulate a sample of subjects

# sample from a multivariate random distribution 
  subjects <- faux::rnorm_multi(
  n = n_subj, 
  mu = 0, # means for random effects are always 0
  sd = c(tau_0, tau_1), # set SDs
  r = rho, # set correlation, see ?faux::rnorm_multi
  varnames = c("T_0s", "T_1s"))

    # add subject IDs
  subjects$subj_id <- seq_len(n_subj) 

  crossing(subjects, items) %>%
    mutate(e_si = rnorm(nrow(.), mean = 0, sd = sigma),
           RT = beta_0 + T_0s + O_0i + (beta_1 + T_1s) * category_contrast + e_si) %>%
    mutate(RT = RT + rexp(nrow(.), rate = 0.001)) %>%
    #mutate(RT = exp(RT)) %>%
    dplyr::select(subj_id, item_id, category, category_contrast, RT)
}


#Use gamme distribution to generate right-skewed data! Or exponentiate the DV!!

```