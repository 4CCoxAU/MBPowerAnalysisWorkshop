---
title: "ManyBabiesPowerAnalysisWorkshopPart1"
author: "Chris Cox"
date: "2024-03-06"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

# Simulating Data for Power Analysis Purposes

```{r, echo=FALSE}
xfun::embed_file('01-introToSimulation.Rmd')
```

## Introduction to the Simulation-Based Approach

Our research question concerns the hyperarticulation hypothesis of infant-directed speech. Do caregivers produce more peripheral vowels in infant-directed speech (IDS) compared to adult-directed speech (ADS)? To answer this question, we manually analysed 9267 extracted vowel tokens from individual caregivers' IDS and ADS. In order eliminate inter-individual differences in formant values that occur due to physiological characteristics, we first z-score-normalised the formant data according to participant and then computed the total area of each caregiver's ADS and IDS vowel space. We used this measure to calculate a standardised mean difference score: Cohen's d.

Let's start by loading the required packages:

```{r setup, include=TRUE}
#if you don't have the pacman package loaded on your computer, uncomment the next line, install pacman, and load in the required packages
#install.packages('pacman')
#load the required packages:
pacman::p_load(knitr, # rendering of .Rmd file
               lme4, # model specification / estimation
               afex, # anova and deriving p-values from lmer
               broom.mixed, # extracting data from model fits
               faux, # generate correlated values
               tidyverse, # data wrangling and visualisation
               ggridges, #visualisation
               viridis, # color schemes for visualisation
               kableExtra, #helps with knitting the .Rmd file
               cowplot, #visualisation tool to include multiple plots
               MASS,
               ggrain)
set.seed(1234)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE, fig.width=12, fig.height=11, fig.fullwidth=TRUE)

plot_theme <- 
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5, size = 18), 
        legend.position = "none", 
        axis.text.x = element_text(size = 13), 
        axis.title.x = element_text(size = 13), 
        axis.text.y = element_text(size = 12), 
        axis.title.y = element_text(size = 13))
```

Let's start with a straightforward example based on MB1. We are interested in the following question: To what extent do infants demonstrate longer looking times to infant-directed speech as opposed to adult-directed speech?

It is easy tot start thinking that the number of subjects and the effect size are the most important factors, or even the only factors, that affect power. Although effect size is often the largest contributor to power, saying it is the only important issue is far from the truth.  There are at least a dozen other factors that can influence the power of a study, and many of these factors should be considered not only from the perspective of doing a power analysis, but also as part of doing good research.  The first couple of factors that we will discuss are more “mechanical” ways of increasing power (e.g., alpha level, sample size and effect size). After that, the discussion will turn to more methodological issues that affect power.

To give an overview of the simulation task, we will simulate data from a design with crossed random factors of subjects and stimuli, fit a model to the simulated data, and then see whether the resulting sample estimates are similar to the population values we specified when simulating the data. In this hypothetical study, subjects classify the emotional expressions of faces as quickly as possible, and we use their response time as the primary dependent variable. Let’s imagine that the faces are of two types: either from the subject’s ingroup or from an outgroup. For simplicity, we further assume that each face appears only once in the stimulus set. The key question is whether there is any difference in classification speed across the type of face.

## Establishing the data-generating parameters

The first thing to do is to set up the parameters that govern the process we assume to give rise to the data, the data-generating process or DGP. Let’s start by defining the sample size: In this hypothetical study, each of 100 subjects will respond to all 50 stimulus items (25 ingroup and 25 outgroup), for a total of 5000 observations.

Note that for independent variables in designs where subjects and stimuli are crossed, you can’t think of factors as being solely “within” or “between” because we have two sampling units; you must ask not only whether independent variables are within- or between- subjects, but also whether they are within- or between- stimulus items. Recall that a within-subjects factor is one where each and every subject receives all of the levels, and a between-subjects factors is one where each subject receives only one of the levels. Likewise, a within-items factor is one for which each stimulus receives all of the levels. For our current example, the ingroup/outgroup factor (category) is within subjects but between items, given that each stimulus item is either ingroup or outgroup.

Now that we have an appropriate structure for our simulated dataset, we need to generate the RT values. For this, we need to establish an underlying statistical model. In this and the next section, we will build up a statistical model step by step, defining variables in the code as we go along that reflect our choices for parameters. For convenience, Table @ref(tab:paramdef) lists all of the variables in the statistical model and their associated variable names in the code.

Let us start with a basic model and build up from there. We want a model of RT for subject 𝑠 and item 𝑖

that looks something like:

𝑅𝑇𝑠𝑖=𝛽0+𝛽1𝑋𝑖+𝑒𝑠𝑖.

According to the formula, response 𝑅𝑇𝑠𝑖for subject 𝑠 and item 𝑖 is defined as sum of an intercept term 𝛽0, which in this example is the grand mean reaction time for the population of stimuli, plus 𝛽1, the mean RT difference between ingroup and outgroup stimuli, plus random noise 𝑒𝑠𝑖. To make 𝛽0 equal the grand mean and 𝛽1 equal the mean outgroup minus the mean ingroup RT, we will code the item category variable 𝑋𝑖as -.5 for the ingroup category and +.5 for the outgroup category.

In the model formula, we use Greek letters (𝛽0
, 𝛽1) to represent population parameters that are being directly estimated by the model. In contrast, Roman letters represent the remaining variables: observed variables whose values are determined by sampling (e.g., 𝑅𝑇𝑠𝑖, 𝑇0𝑠, 𝑒𝑠𝑖) or fixed by the experiment design (𝑋𝑖).

Although this model is incomplete, we can go ahead and choose parameters for 𝛽0 and 𝛽1. For this example, we set a grand mean of 800 ms and a mean difference of 50 ms. You will need to use disciplinary expertise and/or pilot data to choose these parameters; by the end of this tutorial you will understand how to extract those parameters from an analysis.

```{r}
# set fixed effect parameters
beta_0 <- 0 # intercept; i.e., the grand mean
beta_1 <- 0.35 # slope; i.e, effect of category
```

The parameters 𝛽0 and 𝛽1 are fixed effects: they characterize the population of events in which a typical subject encounters a typical stimulus. Thus, we set the mean RT for a “typical” subject encountering a “typical” stimulus to 800 ms, and assume that responses are typically 50 ms slower for outgroup than ingroup faces.

```{r}
# set random effect parameters
tau_0   <- 0.1 # by-subject random intercept sd
omega_0 <- 0.05   # by-item random intercept sd
```


```{r}
# set more random effect and error parameters
tau_1  <-  0.2 # by-subject random slope sd
rho    <-  0.2 # correlation between intercept and slope
sigma  <- 0.5 # residual (error) sd
```

To summarize, we established a reasonable statistical model underlying the data having the form:

𝑅𝑇𝑠𝑖=𝛽0+𝑇0𝑠+𝑂0𝑖+(𝛽1+𝑇1𝑠)𝑋𝑖+𝑒𝑠𝑖

The response time for subject 𝑠
on item 𝑖, 𝑅𝑇𝑠𝑖, is decomposed into a population grand mean 𝛽0, a by-subject random intercept 𝑇0𝑠, a by-item random intercept 𝑂0𝑖, a fixed slope 𝛽1, a by-subject random slope 𝑇1𝑠, and a trial-level residual 𝑒𝑠𝑖. Our data-generating process is fully determined by seven population parameters, all denoted by Greek letters: 𝛽0, 𝛽1, 𝜏0, 𝜏1, 𝜌, 𝜔0, and 𝜎(see Table @ref(tab:paramdef)). In the next section we will apply this data-generating process to simulate the sampling of subjects, items, and trials (encounters).

Simulating the sampling process

Let’s first define parameters related to the number of observations. In this example, we will simulate data from 100 subjects responding to 25 ingroup faces and 25 outgroup faces. There are no between-subject factors, so we can set n_subj to 100. We set n_ingroup and n_outgroup to the number of stimulus items in each condition.

```{r}
# set number of subjects and items
n_subj     <- 100 # number of subjects
n_ingroup  <-  25 # number of ingroup stimuli
n_outgroup <-  25 # number of outgroup stimuli
```


Simulate the sampling of stimulus items

We need to create a table listing each item 𝑖, which category it is in, and its random effect 𝑂0𝑖.

```{r}
# simulate a sample of items
# total number of items = n_ingroup + n_outgroup
items <- data.frame(
  item_id = seq_len(n_ingroup + n_outgroup),
  category = rep(c("IDS", "ADS"), c(n_ingroup, n_outgroup)),
  O_0i = rnorm(n = n_ingroup + n_outgroup, mean = 0, sd = omega_0)
)
```


For the first variable in the dataset, item_id, we have used seq_len() to assign a unique integer to each of the 50 stimulus faces; these IDs function like names. The category variable designates whether the face is ingroup or outgroup, with the first 25 items being ingroup and the last 25 being outgroup. Finally, we sample the values of 𝑂0𝑖 from a normal distribution using the rnorm() function, with a mean of 0 and SD of 𝜔0.

Let us introduce a numeric predictor to represent what category each stimulus item 𝑖 appears in (i.e., for the 𝑋𝑖 in our model). Since we predict that responses to ingroup faces will be faster than outgroup faces, we set ingroup to -0.5 and outgroup to +0.5. We will later multiply this effect coded factor by the fixed effect of category (beta_1 = 50) to simulate data where the ingroup faces are on average -25 ms different from the grand mean, while the outgroup faces are on average 25 ms different from the grand mean. After adding this variable, the resulting table items should look like Table @ref(tab:items-table), although the specific values you obtain for 𝑂0𝑖 may differ, depending on whether you set the random seed.

```{r}
# effect-code category
items$X_i <- recode(items$category, "ADS" = -0.5, "IDS" = +0.5)
```

In R, most regression procedures can handle two-level factors like category as predictor variables. By default, the procedure will create a new numeric predictor that codes one level of the factor as zero and the other as one. Why not just use the defaults? The short explanation is that the default of 0, 1 coding is not well-suited to the kinds of factorial experimental designs often found in psychology and related fields. For the current example, using the default coding for the 𝑋 predictor would change the interpretation of 𝛽0: instead of the grand mean, it would reflect the mean for the group coded as zero. One could change the default, but we feel it is better to be explicit in the code about what values are being used. See https://talklab.psy.gla.ac.uk/tvw/catpred for further discussion; see also the R mixed modeling package afex (Singmann et al. 2019), which provides better defaults for specifying categorical predictors in ANOVA-style designs.

Now we will simulate the sampling of individual subjects, resulting in a table listing each subject and their two correlated random effects. This will be slightly more complicated that what we just did, because we cannot simply sample the 𝑇0𝑠 values from a univariate distribution using rnorm() independently from the 𝑇1𝑠 values. Instead, we must sample ⟨𝑇0𝑠,𝑇1𝑠⟩ pairs—one pair for each subject—from a bivariate normal distribution. To do this, we will use the mvrnorm() function, a multivariate version of rnorm() from the MASS package that comes pre-installed with R. We specify the three parameters describing this distribution—two variances and a correlation—by entering them into a 2x2 variance-covariance matrix using the matrix() function, and then passing this matrix to mvrnorm() using the Sigma argument. This requires converting the standard deviations into variances (by squaring them) and calculating the covariance, which is the product of the correlation and two standard deviations, i.e., 𝜌×𝜏0×𝜏1

.

We only need this one function from MASS, so we can call it directly using the package::function() syntax instead of loading the library (specifically, MASS::mvrnorm() instead of library(MASS)). The resulting table subjects should have the structure shown in Table @ref(tab:subj-table).

An alternative way to sample from a bivariate distribution would be to use the function rnorm_multi() from the faux package (DeBruine 2020), which generates a table of n simulated values from a multivariate normal distribution by specifying the means (mu) and standard deviations (sd) of each variable, plus the correlations (r), which can be either a single value (applied to all pairs), a correlation matrix, or a vector of the values in the upper right triangle of the correlation matrix.

```{r}
# simulate a sample of subjects

# sample from a multivariate random distribution 
subjects <- faux::rnorm_multi(
  n = n_subj, 
  mu = 0, # means for random effects are always 0
  sd = c(tau_0, tau_1), # set SDs
  r = rho, # set correlation, see ?faux::rnorm_multi
  varnames = c("T_0s", "T_1s")
)

# add subject IDs
subjects$subj_id <- seq_len(n_subj)
```

Simulate trials (encounters)

Since all subjects respond to all items, we can set up a table of trials by making a table with every possible combination of the rows in the subject and item tables using the tidyverse function crossing(). Each trial has random error associated with it, reflecting fluctuations in trial-by-trial performance due to unknown factors; we simulate this by sampling values from a normal distribution with a mean of 0 and SD of sigma. The resulting table should correspond to Table @ref(tab:trials-table).

```{r}
# cross subject and item IDs; add an error term
# nrow(.) is the number of rows in the table
trials <- crossing(subjects, items)  %>%
  mutate(e_si = rnorm(nrow(.), mean = 0, sd = sigma)) %>%
  dplyr::select(subj_id, item_id, category, X_i, everything())
```

Calculate the response values

With this resulting table, in combination with the constants beta_0 and beta_1, we have the full set of values that we need to compute the response variable RT according to the linear model we defined above:

𝑅𝑇𝑠𝑖=𝛽0+𝑇0𝑠+𝑂0𝑖+(𝛽1+𝑇1𝑠)𝑋𝑖+𝑒𝑠𝑖

Thus, we calculate the response variable RT by adding together:

    the grand intercept (beta_0),
    each subject-specific random intercept (T_0s),
    each item-specific random intercept (O_0i),
    each sum of the category effect (beta_1) and the random slope (T_1s), multiplied by the numeric predictor (X_i), and
    each residual error (e_si).

After this we will use dplyr::select() to keep the columns we need. Note that the resulting table (Table @ref(tab:dat-sim-table)) has the structure that we set as our goal at the start of this exercise, with the additional column X_i, which we will keep to use in the estimation process, described in the next section.

```{r}
# calculate the response variable
dat_sim <- trials %>%
  mutate(RT = beta_0 + T_0s + O_0i + (beta_1 + T_1s) * X_i + e_si) %>%
  dplyr::select(subj_id, item_id, category, X_i, RT)
```


To make it easier to try out different parameters or to generate many datasets for the purpose of power analysis, you can put all of the code above into a custom function. Set up the function to take all of the parameters we set above as arguments. We’ll set the defaults to the values we used, but you can choose your own defaults. The code below is just all of the code above, condensed a bit. It returns one dataset with the parameters you specified.


```{r}
# set up the custom data simulation function
my_sim_data <- function(
  n_subj = 50,   # number of subjects
  ADS_n  = 10,   # number of ingroup stimuli
  IDS_n =  10,   # number of outgroup stimuli
  beta_0 = 0,   # grand mean
  beta_1 =  0.35,   # effect of category
  omega_0 =  0.1,   # by-item random intercept sd
  tau_0 = 0.1,   # by-subject random intercept sd
  tau_1 =  0.06,   # by-subject random slope sd
  rho = 0.2,   # correlation between intercept and slope
  sigma = 0.5) { # residual (standard deviation)

  items <- data.frame(
    item_id = seq_len(ADS_n + IDS_n),
    category = rep(c("ADS", "IDS"), c(ADS_n, IDS_n)),
    category_contrast = rep(c(-0.5, 0.5), c(ADS_n, IDS_n)),
    O_0i = rnorm(n = ADS_n + IDS_n, mean = 0, sd = omega_0))

  # simulate a sample of subjects

# sample from a multivariate random distribution 
  subjects <- faux::rnorm_multi(
  n = n_subj, 
  mu = 0, # means for random effects are always 0
  sd = c(tau_0, tau_1), # set SDs
  r = rho, # set correlation, see ?faux::rnorm_multi
  varnames = c("T_0s", "T_1s"))

    # add subject IDs
  subjects$subj_id <- seq_len(n_subj) 

  crossing(subjects, items) %>%
    mutate(e_si = rnorm(nrow(.), mean = 0, sd = sigma),
           RT = beta_0 + T_0s + O_0i + (beta_1 + T_1s) * category_contrast + e_si) %>%
    dplyr::select(subj_id, item_id, category, category_contrast, RT)
}
```

Now you can generate a dataset with the default parameters using my_sim_data() or, for example, a dataset with 500 subjects and no effect of category using my_sim_data(n_subj = 500, beta_1 = 0).

RT ~ 1 + X_i + (1 | item_id) + (1 + X_i | subj_id)

* RT is the response;
* 1 corresponds to the grand intercept (beta_0);
* X_i is the predictor for the ingroup/outgroup manipulation for item i;
* (1 | item_id) specifies a by-subject random intercept (O_0i);
* (1 + X_i | subj_id) specifies a subject-specific random intercept (T_0s) plus the subject-specific random slope of category (T_1s).

The error term (e_si) is automatically included in all models, so is left implicit. The ‘fixed’ part of the formula, RT ~ 1 + X_i, establishes the 𝑅𝑇𝑠𝑖+𝛽0+𝛽1𝑋𝑖+𝑒𝑠𝑖 part of our linear model. Every model has an intercept (𝛽0) term and residual term (𝑒𝑠𝑖

) by default, so you could alternatively leave the 1 out and just write RT ~ X_i.

The terms in parentheses with the “pipe” separator (|) define the random effects structure. For each of these bracketed terms, the left-hand side of the pipe names the effects you wish to allow to vary and the right hand side names the variable identifying the levels of the random factor over which the terms vary (e.g., subjects or items). The first term, (1 | item_id) allows the intercept (1) to vary over the random factor of items (item_id). This is an instruction to estimate the parameter underlying the O_0i values, namely omega_0. The second term, (1 + X_i | subj_id), allows both the intercept and the effect of category (coded by X_i) to vary over the random factor of subjects (subj_id). It is an instruction to estimate the three parameters that underlie the T_0s and T_1s values, namely tau_0, tau_1, and rho.

Interpreting the lmer summary

The other arguments to the lme4 function are the name of the data frame where the values are found (dat_sim). Because we loaded in lmerTest after lme4, the 𝑝
-values are derived using the Satterthwaite approximation, for which the default estimation technique in lmer()—restricted likelihood estimation (REML = TRUE)—is the most appropriate (Luke 2017). Use the summary() function to view the results.

```{r}
dat_sim <- my_sim_data(n_subj = 30)

dat_sim %>%
  mutate(subj_id = as.factor(subj_id)) %>%
  ggplot() +
  geom_density_ridges(aes(y = reorder(subj_id, RT), x = RT, color = "black", fill = subj_id), show.legend = FALSE, quantile_lines = T, quantiles = c(0.025, 0.5, 0.975), alpha = 0.6, scale = 1.1) +
  ggtitle('Intercept Variability by Subject') +
  geom_vline(xintercept = 0, color = "black", size = 0.3, linetype = "dotted") +
  xlab('DV') +
  ylab('Subject ID') +
  #facet_wrap(~category) +
  scale_color_manual(values=c(viridis(n = 30))) +
  scale_fill_manual(values=c(viridis(n = 30))) +
  plot_theme

dat_sim %>%
  mutate(item_id = as.factor(item_id)) %>%
  ggplot() +
  geom_density_ridges(aes(y = reorder(item_id, RT), x = RT, color = "black", fill = item_id), show.legend = FALSE, quantile_lines = TRUE, quantiles = c(0.025, 0.5, 0.975), alpha = 0.6, scale = 1.1) +
  ggtitle('Intercept Variability by Item') +
  #geom_vline(xintercept = 800, color = "black", size = 0.3, linetype = "dotted") +
  xlab('DV') +
  ylab('Item ID') +
  #facet_wrap(~category) +
  scale_color_manual(values=c(viridis(n = 30))) +
  scale_fill_manual(values=c(viridis(n = 30))) +
  plot_theme
```


```{r}
dat_sim <- my_sim_data(n_subj = 50)
meanADSRT <- mean(filter(dat_sim, category == "ADS")$RT)

dat_sim_plot <- dat_sim %>%
  group_by(subj_id, category) %>%
  dplyr::summarise(medRT = mean(RT)) %>%
  dplyr::mutate(medRT = medRT - meanADSRT)

dat_sim_plot %>%
  group_by(category) %>%
  dplyr::summarise(mean(medRT))

ggplot(aes(x = category, y = medRT, fill = category), data = dat_sim_plot) + 
  geom_rain(alpha = 0.8, rain.side = "f1x1", id.long.var = "subj_id", point.args.pos = list(position = position_jitter(width = 0.04, height = 0, seed = 42)), line.args.pos = list(position = position_jitter(width = 0.04, height = 0, seed = 42))) + 
  scale_fill_manual(values = c("#FC4E07", "steelblue")) + 
  ggtitle('Effect Size Differences across Speech Styles') + 
  xlab("Speech Style") + 
  ylab('Effect Sizes') + 
  scale_color_manual(values = viridis(n = 27)) +
  plot_theme
```


```{r}
# fit a linear mixed-effects model to data
mod_sim <- lmer(RT ~ 1 + category_contrast + (1 | item_id) + (1 + category_contrast | subj_id), data = dat_sim)

summary(mod_sim, corr = FALSE)
```



```{r}
# simulate, analyze, and return a table of parameter estimates
single_run <- function(...) {
  # ... is a shortcut that forwards any arguments to 
  # my_sim_data(), the function created above
  dat_sim <- my_sim_data(...)
  mod_sim <- lmer(RT ~ category_contrast + (1 | item_id) + (1 + category_contrast | subj_id),
                dat_sim)
  
  broom.mixed::tidy(mod_sim)
}
single_run()

# run one model with new parameters
single_run(n_subj = 200, beta_1 = 0.1)

# run simulations and save to a file
n_runs <- 100 # use at least 1000 to get stable estimates
sims <- purrr::map_df(1:n_runs, ~ single_run())
write_csv(sims, "sims.csv")

# read saved simulation data
sims <- read_csv("sims.csv", col_types = cols(
  # makes sure plots display in this order
  group = col_factor(ordered = TRUE),
  term = col_factor(ordered = TRUE)
  ))

sims %>%
  filter(effect == "fixed") %>%
  dplyr::select(term, estimate, p.value)

# calculate mean estimates and power for specified alpha
alpha <- 0.05

sims %>% 
  filter(effect == "fixed") %>%
  group_by(term) %>%
  dplyr::summarize(
    mean_estimate = mean(estimate),
    mean_se = mean(std.error),
    power = mean(p.value < alpha),
    .groups = "drop"
  )
```


Exercise 1: Let's explore the effect of repeated measures on power. Try to run a power analysis with each subject receiving two items and another power analysis with each subject receiving 15 items. What happens to the estimate of statistical power?

Write your answer here:


______________________________________________________________________________________________________________________



Exercise 2: How would you add a positive effect of age as a predictor to the model (hint: it involves randomly sampling age for each child and adding an effect to the simulation code and model)?


______________________________________________________________________________________________________________________



```{r}
# set up the custom data simulation function
my_sim_data <- function(
  n_subj = 50,   # number of subjects
  ADS_n  = 10,   # number of ingroup stimuli
  IDS_n =  10,   # number of outgroup stimuli
  beta_0 = 0,   # grand mean
  beta_1 =  0.35,   # effect of category
  beta_as = 0.8,
  subject_as = 0.05,
  omega_0 =  0.1,   # by-item random intercept sd
  tau_0 = 0.1,   # by-subject random intercept sd
  tau_1 =  0.06,   # by-subject random slope sd
  rho = 0.2,   # correlation between intercept and slope
  sigma = 0.5) { # residual (standard deviation)

  items <- data.frame(
    item_id = seq_len(ADS_n + IDS_n),
    category = rep(c("ADS", "IDS"), c(ADS_n, IDS_n)),
    category_contrast = rep(c(-0.5, 0.5), c(ADS_n, IDS_n)),
    O_0i = rnorm(n = ADS_n + IDS_n, mean = 0, sd = omega_0)) %>%
    mutate(item_id = faux::make_id(nrow(.), "I"))

  # simulate a sample of subjects

# sample from a multivariate random distribution 
  subjects <- faux::rnorm_multi(
  n = n_subj, 
  mu = 0, # means for random effects are always 0
  sd = c(tau_0, tau_1, subject_as), # set SDs
  r = rho, # set correlation, see ?faux::rnorm_multi
  varnames = c("T_0s", "T_1s", "S_as")) %>%
    mutate(subj_id = faux::make_id(nrow(.), "S")) %>%
    mutate(X_a = runif(n_subj, min = -0.5, max = 0.5))
#add subject age measure, sample from distribution from -0.5 to 0.5.
  crossing(subjects, items) %>%
    mutate(e_si = rnorm(nrow(.), mean = 0, sd = sigma),
           RT = beta_0 + T_0s + O_0i + (beta_1 + T_1s) * category_contrast + ((beta_as + S_as) * X_a * category_contrast) + e_si) %>%
    dplyr::select(subj_id, item_id, category, category_contrast, RT, X_a)
}
```

```{r}
dat_sim <- my_sim_data()

dat_sim %>%
ggplot() + 
  geom_point(aes(y = RT, x = X_a, color = subj_id), position = "jitter", alpha = 0.6, size = 1) + 
  geom_smooth(method = "lm", se = TRUE, formula = y ~ x, aes(y = RT, x = X_a)) +
  ggtitle("Age") +
  xlab("Age") + 
  facet_wrap(~category_contrast) +
  plot_theme

dat_sim <- my_sim_data()
mod_sim <- lmer(RT ~ 1 + category_contrast*X_a + (1 | item_id) + (1 + category_contrast | subj_id), data = dat_sim)
summary(mod_sim, corr = FALSE)
```



Mixed-effects modeling is a powerful technique for analyzing data from complex designs. The technique is close to ideal for analyzing data with crossed random factors of subjects and stimuli: it gracefully and simultaneously accounts for subject and item variance within a single analysis, and outperforms traditional techniques in terms of type I error and power (Barr et al. 2013). However, this additional power comes at the price of technical complexity.

Here we only considered a design with a normally distributed response variable. However, generalised linear mixed effect models allow for response variables with different distributions, such as binomial. Our supplemental online materials (https://osf.io/3cz2e/) illustrate the differences in simulation required for the study design in this paper with a binomial accuracy score (correct/incorrect) as the response variable.


